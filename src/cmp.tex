\chapter[Сравнение алгоритмов обучения ранжированию]{Сравнение алгоритмов обучения\\ранжированию}

\section{Описание данных}

Сравнение алгоритмов производилось по наборам данных таких коллекций как: AOL, LETOR 3, LETOR 4, MSLR, WCL2R, Yahoo! Learning to Rank Challenge. В датасетах не содержатся  сами документы, а только векторы их признаков. Таким образом, любая разница в производительности ранжирования обусловлена только алгоритмом ранжирования. Также в датасетах присутствуют сами запросы соответствующие документам.  

LETOR 3 состоит из двух частей: <<Gov>> и OHSUMED. В <<Gov>> содержится около одного миллиона документов. В нём определены три задачи поиска: определение темы (TD), поиск домашней страницы (HP) и поиск именованной страницы (NP). Определение темы направлено на поиск списка
точек входа на веб-сайты, в основном посвященные данной теме, то есть сайтам содержащим ссылки на искомые страницы. Поиск домашней страницы направлен на возврат некоторой главной страницы, соответствующей запросу. Поиск именованной страницы заключается в поиске страницы, название которой идентично запросу. В принципе, существует только один ответ для поиска домашней страницы и поиска именованной страницы. OHSUMED является подмножеством базы данных о медицинских публикациях MEDLINE. Он состоит примерно из 0,3 миллиона записей из 270 медицинских журналов. Поля записи включают название, аннотацию, ключевые слова, автора, источник и тип публикации~\cite{LETOR3}. 

LETOR 4 содержит два набора данных (MQ2007, MQ2008) для четырех настроек ранжирования: контролируемое, полу-контролируемое, ранжирование по списку и агрегирование рангов~\cite{AOL}.

Набор данных AOL был создан на основе логов коммерческой поисковой системы. Логи
представляют собой выборку поисковой активности 658000 анонимизированных пользователей из США за трехмесячный период. Набор данных содержит 4,8 миллиона запросов и 1,8 миллиона URL-адресов. В
этом наборе данных предполагается, что релевантность документа для запроса пропорциональна количеству раз, когда пользователи кликали не него~\cite{AOL}.

Два набора данных Yahoo происходят из разных регионов. Первый соответствует США, другой  - азиатским странам. Оба набора данных фактически являются подмножеством всего обучающего набора, используемого внутри компании для обучения функций ранжирования поисковой системы <<Yahoo!>>~\cite{Yahoo}.

Набор WCL2R создан на основе логов чилийской поисковой системы TodoCL. Данные собирались в течении 10 месяцев~\cite{WCL2R}.

Наборы данных MSLR-WEB10K состоит из 10000 запросов. Наборы данных состоят из векторов объектов, извлеченных из пар <<запрос -- URL>>, а также меток оценки релевантности. Оценки получены на основе устаревшего набора меток коммерческой поисковой системы в Интернете Bing~\cite{MSLR}.

Набор данных Istella использовался в прошлом для обучения одного из этапов конвейера ранжирования Stellar production. Является одним из самых больших общедоступных наборов данных. Полный набор данных состоит из 33018 запросов и 220 объектов, представляющих каждую пару запрос--документ. Он состоит из 10454629 примеров, помеченных значениями релевантности~\cite{istella}.

\section{Критерии сравнения}

Наиболее распространенный подход к оценке эффективности ранжирующего отображения основан на проверке критериев точности и полноты поиска, так как эти факторы и являются основными требованиями предъявляемыми к поисковым системам~\cite{metrics}. Поэтому в качестве критериев сравнения в данной работе используются метрики MAP и NDCG.

Рассмотрим множество документов $D$. Для каждого запроса $q$ результатом работы алгоритма ранжирования будет отображение $f: d \to R$, ставящее в соответствие каждой странице ей рейтинг, который тем больше, чем более документ $d$ релевантен к запросу $q$. Для оценки качества ранжирования нужен эталон $r_t(d)$, с которым будут сравниваться результаты.
 
MAP --- метрика средней точности нахождения релевантных документов. Она применяется в том случае, если $r_t(d)$ принимает бинарные значения, т.~е. каждый документ полностью  релевантен либо нет. Данная метрика была выбрана, так как она непосредственно отражает точность ранжирующей модели.

Precision at $N$ или точность на $N$ элементах позволяет оценить долю релевантных документов среди первых $N$ элементов ранжированного списка и рассчитывается по формуле:
\begin{equation}
	\label{eq:map1}
	p @ N=\frac{1}{N} \sum_{k=1}^N r_t(P^{\prime}(k)),
\end{equation}
где $P^{\prime}$ --- обратная перестановка, то есть $P^{\prime}(k)$ --- документ на $k$-ом месте после ранжированя.

Average precision at $N$ позволяет учесть место, на котором оказался документ в ранжированном списке. Гораздо важнее увидеть релевантный документ на 1-ой позиции, чем в конце списка. Формула качества принимает
следующий вид: 
\begin{equation}
	\label{eq:map2}
	ap @ N=\frac{1}{N} \sum_{k=1}^N [r_t(P^{\prime}(k)) \cdot p@k].
\end{equation}
Данная величина уже зависит от порядка. Она достигает максимума, если все релевантные документы находятся вверху ранжированного списка.

Mean average precision at $N$, в отличии от предыдущих метрик вычисляется сразу для всех запросов множества $Q$ и является их средним. Пусть $|Q| = K$, тогда метрика записывается как:
\begin{equation}
	\label{eq:map3}
	map @ N=\frac{1}{K} \sum_{j=1}^K ap @ N_j.
\end{equation}

Цель метрики MAP аналогична NDCG, но в отличии от предыдущей группы метрик, рассмотренной выше, NDCG можно использовать и при небинарных значениях $r_t$, то есть позволяет учесть что какой-то элмент может быть более или менее релевантным, чем другой. Эта метрика была выбрана, так как она позволяет одновременно учитывать порядок и релевантность документов.

Cumulative gain at $N$. В простейшем случае суммируем все значения релевантностей документов среди N первых.
\begin{equation}
	\label{eq:ndcg1}
	CG @ N= \sum_{k=1}^N r_t(P^{\prime}(k)).
\end{equation}

Discounted cumulative gain at $N$. Модификация позволяет учесть порядок элементов в топе.
\begin{equation}
	\label{eq:ndcg2}
	D C G @ N=\sum_{k=1}^N \frac{2^{r_t(P^{\prime}(k))}-1}{\log _2(k+1)}.
\end{equation}
В данной метрике чем более релевантен документ, тем больше числитель. Знаменатель штрафует за позицию документа в списке. Если документ очень релевантен, но занимает низкую позицию, то штраф будет большим, иначе маленьким. Метрика достигает максимума, если все релевантные документы находятся в начале поисковой выдачи.

Normalized discounted cumulative gain at $N$. Призвана нормализовать результаты предыдущей метрики.
\begin{equation}
	\label{eq:ndcg3}
	nD C G @ N=\frac{D C G @ N}{maxD C G @ N},
\end{equation}
где $maxD C G @ N$ --- значение метрик при идеальном ранжировании.

Рассмотренные метрики позволяют с высокой точностью проанализировать результаты ранжирования. Формулы~(\ref{eq:map1})--(\ref{eq:ndcg3})~\cite{metrics}.

В таблицах \ref{tbl:LRM}~--~\ref{tbl:ListNetM} приведены значения метрик рассматриваемых алгоритмов на различных наборах данных.  Значения для таблицы \ref{tbl:LRM} приведены из работ Тао Квина~\cite{LETOR3}, Чена Ванга~\cite{LRM2}, Максима Волкова~\cite{LRM3}. Значения для таблицы \ref{tbl:RSVMM} приведены из работ Тао Квина~\cite{LETOR3}, Антонио Френо~\cite{AOL}, Роберта Буса--Факете~\cite{RSVMM1}, Тизиано Папини~\cite{RSVMM2}. Значения для таблицы \ref{tbl:LambdaRankM} приведены из работ Тизиано Папини~\cite{RSVMM2}, Хинуй Дая~\cite{LDRM1}, Минг Тана~\cite{LDRM2}. Значения для таблицы \ref{tbl:ListNetM} приведены из работ Тао Квина ~\cite{LETOR3}, Хаи--Тао Ю~\cite{LNM1}.

\begin{table}[!ht]
	\begin{center}
		\begin{threeparttable}
			\captionsetup{justification=raggedright,singlelinecheck=off}
			\caption{Значение метрик для алгоритма Linear Regression}
			\label{tbl:LRM}
			\begin{tabular}{|l|c|c|c|c|}
				\hline
				\makecell[c]{Датасет}& NDCG@3 & NDCG@5 & NDCG@10 & MAP \\\hline
				TD2003&	0,3071&	0,2984&	0,33&	0,2409\\\hline
				TD2004&	0,3352&	0,3257&	0,30&	0,2078\\\hline
				NP2003&	0,6135&	0,6423&	0,665&	0,5644\\\hline
				NP2004&	0,5554&	0,6135&	0,653&	0,5142\\\hline
				HP2003&	0,5097&	0,5463&	0,594&	0,4968\\\hline
				HP2004&	0,5752&	0,613&	0,646&	0,5256\\\hline
				OHSUMED&	0,4426&	0,4278&	0,411&	0,422\\\hline
				MQ2007&	0,3935&	0,4111&	0,4288&	0,4497\\\hline
				MQ2008&	0,4289&	0,4773&	-&	-\\\hline
				Среднее& 0,4623&	0,4839&	0,5034&	0,4277\\\hline
			\end{tabular}
		\end{threeparttable}
	\end{center}
\end{table}

\begin{table}[!ht]
	\begin{center}
		\begin{threeparttable}
			\captionsetup{justification=raggedright,singlelinecheck=off}
			\caption{Значение метрик для алгоритма Ranking SVM}
			\label{tbl:RSVMM}
			\begin{tabular}{|l|c|c|c|c|}
				\hline
				\makecell[c]{Датасет}  & NDCG@3 & NDCG@5 & NDCG@10 & MAP \\\hline
				TD2003&	0,3441&	0,3621&	0,346&	0,2628\\\hline
				TD2004&	0,3467&	0,324&	0,307&	0,2237\\\hline
				NP2003&	0,7654&	0,7823&	0,8&	0,6957\\\hline
				NP2004&	0,7503&	0,7957&	0,806&	0,6588\\\hline
				HP2003&	0,7749&	0,7954&	0,807&	0,7408\\\hline
				HP2004&	0,7147&	0,7512&	0,768&	0,6675\\\hline
				OHSUMED&	0,4207&	0,4164&	0,414&	0,4334\\\hline
				MQ2007&	0,40628&	0,41426&	0,44386&	0,46448\\\hline
				MQ2008&	0,42858&	0,46954&	0,22792&	0,46956\\\hline
				MSLR-WEB10K&	-&	-&	0,735&	0,498\\\hline
				AOL&	0,603&	0,647&	0,705&	-\\\hline
				Yahoo1&	-&	-&	0,803&	0,702\\\hline
				Yаhoo2&	-&	-&	0,747&	-\\\hline
				Istella&	-&	-&	0,808&	0,773\\\hline
				WCL2R&	0,353&	-&	0,395&	0,432\\\hline
				Среднее& 0,5371&	0,5758&	0,6075&	0,5401\\\hline
			\end{tabular}
		\end{threeparttable}
	\end{center}
\end{table}

\clearpage

\begin{table}[!ht]
	\begin{center}
		\begin{threeparttable}
			\captionsetup{justification=raggedright,singlelinecheck=off}
			\caption{Значение метрик для алгоритма LambdaRank}
			\label{tbl:LambdaRankM}
			\begin{tabular}{|l|c|c|c|c|}
				\hline
				\makecell[c]{Датасет} & NDCG@3 & NDCG@5 & NDCG@10 & MAP \\\hline
				OHSUMED	&0,4942&	0,478&	0,4503&	-\\\hline
				MSLR-WEB10K&0,4498&	0,4528&	0,60225&	0,498\\\hline
				AOL&0,596&	0,644&	0,7&	-\\\hline
				Yahoo1&	-&	-&	0,802&	0,7\\\hline
				Istella&-&	-&	0,81&	0,776\\\hline
				Среднее& 0,5133	&0,5249	&0,6729	&0,6580\\\hline
			\end{tabular}
		\end{threeparttable}
	\end{center}
\end{table}

\begin{table}[!ht]
	\begin{center}
		\begin{threeparttable}
			\captionsetup{justification=raggedright,singlelinecheck=off}
			\caption{Значение метрик для алгоритма ListNet}
			\label{tbl:ListNetM}
			\begin{tabular}{|l|c|c|c|c|}
				\hline
				\makecell[c]{Датасет}  & NDCG@3 & NDCG@5 & NDCG@10 & MAP \\\hline
				TD2003&	0,3365&	0,3393&	0,348&	0,2753\\\hline
				TD2004&	0,3573&	0,3325&	0,317&	0,2231\\\hline
				NP2003&	0,7579&	0,7843&	0,801&	0,6895\\\hline
				NP2004&	0,7587&	0,7965&	0,812&	0,672\\\hline
				HP2003&	0,8128&	0,8298&	0,837&	0,7659\\\hline
				HP2004&	0,7213&	0,7694&	0,784&	0,6899\\\hline
				OHSUMED&0,4732&	0,4432&	0,441&	0,4457\\\hline
				MQ2007&	0,4091&	0,417&	0,444&	0,4652\\\hline
				MQ2008&	0,4324&	0,4747&	0,2303&	0,4775\\\hline
				MSLR-WEB10K&0,4324&	0,4349&	0,45&	-\\\hline
				Среднее& 0,5492	&0,5622	&0,5464	&0,5227\\\hline
			\end{tabular}
		\end{threeparttable}
	\end{center}
\end{table}

Из таблиц \ref{tbl:LRM}, \ref{tbl:RSVMM}, \ref{tbl:ListNetM} видно, что алгоритмы Linear Regression, Ranking SVM, ListNet соответственно лучше всего справляются с датасетами NP и HР, то есть способны хорошо находить сайты, имеющие заголовок, схожий с запросом. Хуже всего он справляются с датасетами TD, плохо ранжируют сайты, соответствующие тематике запроса. Согласно таблице \ref{tbl:LambdaRankM} алгоритм LambdaRank справляется примерно одинаково со всеми датасетами, на которых он тестировался.

В таблицах \ref{tbl:LRM}--\ref{tbl:ListNetM} для алгоритмов Linear Regression, Ranking SVM, LambdaRank, ListNet соответственно можно наблюдать рост значения метрики NDCG с увеличением $N$. Из этого можно сделать вывод, что они не лучшим образом ранжируют страницы, находящиеся в самом топе поисковой выдачи. То есть страницу в топ 10 будут расположены ближе к эталону, чем в топ 3. Это особенно заметно у алгоритма LambdaRank. А вот алгоритм ListNet на некоторых наборах справляется с ранжированием первых 5 элементов, лучше чем с первых 10.

Точность не будет расти постоянно с увеличением $N$. Это видно из значений метрики MAP. Она показывает, что точность определения релевантности элементов, находящихся далеко от топа, тоже не велика. Хотя это не является проблемой, потому что порядок элементов, плохо подходящих по запросу, не имеет большого значения и алгоритмы не уделяют большего внимания таким документам.

\section{Сравнение алгоритмов}
Для оценки общей эффективности обучения методам ранжирования используется показатель <<выигрышное число>>~\cite{cmp}. Оно определяется как количество алгоритмов, которое выбранный алгоритм может превзойти на наборе датасетов, или более формально:
\begin{equation}
	\label{eq:wn1}
	\mathrm{WN}_i(M)=\sum_{j=1}^n \sum_{k=1}^m I_{\{M_i(j)>M_k(j)\}},
\end{equation}
где $j$ --- номер набора данных, $n$ --- кол-во наборов данных, $i$, $k$ --- индексы алгоритмов, $M_i(j)$ --- производительность $i$-го алгоритма на $j$-ом наборе данных и $I_{\{M_i(j)>M_k(j)\}}$ --- индикаторная функция, такая что
\[
I_{\{M_i(j)>M_k(j)\}}= \begin{cases}1 & \text { если оба определены и } M_i(j)>M_k(j), \\ 0 & \text { иначе. }\end{cases}
\]

Будем использовать нормализованное <<выигрышное число>>, для удобства интерпретации результатов, определённое как:
\begin{equation}
	\label{eq:wn2}
	NWN_i(M)=\frac{WN_i(M)}{IWN_i(M)},
\end{equation}
где $IWN_i(M)$ --- идеальное <<выигрышное число>>, то есть теоретически наибольшее число, которое было
бы у алгоритма в случае, если бы он был наиболее точным. Определяется как:
\begin{equation}
	\label{eq:wn3}
	\mathrm{IWN}_i(M)=\sum_{j=1}^n \sum_{k=1}^m D_{\{M_i(j)>M_k(j)\}},
\end{equation}
где индикаторная функция имеет значение
\[
D_{\{M_i(j)>M_k(j)\}}= \begin{cases}1 & \text { если оба определены }, \\ 0 & \text { иначе }.\end{cases}
\]

Формулы~(\ref{eq:wn1})--(\ref{eq:wn3})~\cite{cmp}.

Исходные данные взяты из таблиц \ref{tbl:LRM}~--~\ref{tbl:ListNetM}. Результаты сравнения алгоритмов посчитаны по методике изложенной выше и приведены в таблице~\ref{tbl:measure}. Данная таблица показывает <<выигрышное число>> и количество наборов данных, используемых для сравнения, конкретного алгоритма для различных метрик ранжирования.

\begin{table}[h]
	\begin{center}
		\begin{threeparttable}
			\captionsetup{justification=raggedright,singlelinecheck=off}
			\caption{Результаты замеров производительности методов обучения ранжированию}
			\label{tbl:measure}
			\begin{tabular}{|l|c|c|c|c|c|c|c|c|}
				\hline
				\makecell[c]{\multirow{2}{*}{Метод}}  &\multicolumn{2}{c|}{NDCG@3} & \multicolumn{2}{c|}{NDCG@5} & \multicolumn{2}{c|}{NDCG@10} & \multicolumn{2}{c|}{MAP} \\
			    \cline{2-9}  & NWN  & к.д & NWN  & к.д & NWN  & к.д.  & NWN  & к.д. \\\hline
				Linear Regression &	0,1053&	9&	0,2105&	9&	0&	8&	0&	8 \\\hline
				Ranking SVM &0,5000&	11&	0,4000&	10&	0,5217&	16&	0,5500&	13 \\\hline
				LambdaRank & 0,8000&	3&	0,8000&	3&	0,6250&	5&	0,3333&	3 \\\hline
				ListNet  &	0,8000&	10&	0,8000&	10&	0,8500&	10&	0,8824&	9 \\\hline
			\end{tabular}
		\end{threeparttable}
	\end{center}
\end{table}


\addtocontents{toc}{\protect\setcounter{tocdepth}{0}}
\section{Вывод}
\addtocontents{toc}{\protect\setcounter{tocdepth}{2}}

В рамках выбранных методов лучше всех себя показал ListNet, так как его <<выигрышные числа>> самые высокие. Чуть хуже показал себя алгоритм LambdaRank, а именно он хуже ранжирует документы находящиеся не в топе поисковой выдачи. Алгоритм Ranking SVM обладает хорошим <<выигрышным числом>> для метрики MAP, то есть достаточно неплохо ранжирует полный список документов. Алгоритм Linear Regression оказался самым худшим по всем метрикам.

Из таблицы~\ref{tbl:measure} можно сделать вывод, что поточенный подход является самым простым, но наименее эффективным. Списочный подход является его противоположностью. Попарный подход является нечто средним. Из сравнения алгоритмов LambdaRank и ListNet видно, что метод с попарныйм подходом может ранжировать элементы в топе выдачи, от которых и требуется наибольшая точность, также хорошо как и списочный алгоритм, но является проще в разработке и обучении.