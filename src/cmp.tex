\chapter[Сравнение алгоритмов обучения ранжированию]{Сравнение алгоритмов обучения\\ранжированию}

\section{Критерии сравнения}

Наиболее распространенный подход к оценке эффективности ранжирующего отображения основан на проверке критериев точности и полноты поиска, так как они и являются основными требованиями к поисковым системам~\cite{metrics}. Поэтому в качестве критериев сравнения в данной работе используются метрики MAP и NDCG.

Рассмотрим множество документов $D$. Для каждого запроса $q$ результатом работы алгоритма ранжирования будет отображение $f: d \to R$, ставящее в соответствие каждой странице ей рейтинг, который тем больше, чем больше документ $d$ подходит к запросу $q$. Для оценки качества ранжирования нужен эталон $r_t(d)$, с которым будут сравниваться результаты.
 
MAP --- метрика средней точности нахождения релевантных документов. Данная метрика применяется в том случае, если $r_t(d)$ принимает бинарные значения, т.~е. каждый документ полностью либо релевантен либо нет. Данная метрика была выбрана, так как она непосредственно отражает точность ранжирующей модели.

Precision at $N$ или точность на $N$ элементах позволяет оценить долю релевантных документов среди первых $N$ элементов ранжированного списка и рассчитывается по формуле:
\begin{equation}
	\label{eq:map1}
	p @ N=\frac{1}{N} \sum_{k=1}^N r_t(P^{\prime}(k)),
\end{equation}
где $P^{\prime}$ --- обратная перестановка, то есть $P^{\prime}(k)$ --- документ на $k$-ом месте после ранжированя.

Average precision at $N$ позволяет учесть место, на котором оказался документ в ранжированном списке. Гораздо важнее увидеть релевантный документ на 1-ой позиции, чем в конце списка. Формула качества принимает
следующий вид: 
\begin{equation}
	\label{eq:map2}
	ap @ N=\frac{1}{N} \sum_{k=1}^N [r_t(P^{\prime}(k)) \cdot p@k].
\end{equation}
Данная величина уже зависит от порядка. Она достигает максимума, если все релевантные документы находятся вверху ранжированного списка.

Mean average precision at $N$, в отличии от предыдущих метрик вычисляется сразу для все запросов множества $Q$ и является их средним. Пусть $|Q| = K$, тогда метрика записывается как:
\begin{equation}
	\label{eq:map3}
	map @ N=\frac{1}{K} \sum_{j=1}^K ap @ N_j.
\end{equation}

В отличии от предыдущей группы метрик рассмотренные ниже NDCG можно использовать и при небинарных значениях $r_t$. Эта метрика была выбрана, так как она позволяет учитывать порядок и релевантность документов.

Cumulative gain at $N$. В простейшем случае суммируем все значения релевантностей документов среди N первых.
\begin{equation}
	\label{eq:ndcg1}
	CG @ N= \sum_{k=1}^N r_t(P^{\prime}(k)).
\end{equation}

Discounted cumulative gain at $N$. Модификация позволяет учесть порядок элементов в топе.
\begin{equation}
	\label{eq:ndcg2}
	D C G @ N=\sum_{k=1}^N \frac{2^{r_t(P^{\prime}(k))}-1}{\log _2(k+1)}.
\end{equation}
В данной метрике чем более релевантен документ, тем больше числитель. Знаменатель зависит от позиции документа, он штрафует за позицию документа в списке. Если документ очень релевантен, но занимает низкую позицию, то штраф будет большим, иначе маленьким. метрика достигает максимума, если все релевантные документы находятся в топе поисковой выдачи.

Normalized discounted cumulative gain at $N$. Призвана нормализовать результаты предыдущей метрики.
\begin{equation}
	\label{eq:ndcg3}
	nD C G @ N=\frac{D C G @ N}{maxD C G @ N},
\end{equation}
где $maxD C G @ N$ --- значение метрик при идеальном ранжировании.

Рассмотренные метрики, являясь довольно простыми для восприятия и позволяют с высокой точностью проанализировать результаты ранжирования. Формулы~(\ref{eq:map1})--(\ref{eq:ndcg3})~\cite{metrics}.

\section{Сравнение алгоритмов}

Сравнение алгоритмов производилось по наборам данных таких коллекций как: AOL, LETOR 2.0, LETOR 3.0, LETOR 4.0, MSLR, WCL2R, Yahoo! Learning to Rank Challenge, Yandex Internet Mathematics 2009 contest. В самих датасетах содержатся не документы, а векторы их признаков. Таким образом, любая разница в производительности ранжирования обусловлена алгоритмом ранжирования, а не используемыми функциями.

Для оценки общей эффективности обучения методам ранжирования используется показатель <<выигрышное число>>~\cite{cmp}. Оно определяется как количество алгоритмов, которое алгоритм может превзойти на наборе датасетов, или более формально:
\begin{equation}
	\label{eq:wn1}
	\mathrm{WN}_i(M)=\sum_{j=1}^n \sum_{k=1}^m I_{\{M_i(j)>M_k(j)\}},
\end{equation}
где $j$ --- номер набора данных, $n$ --- кол-во наборов данных, $i$, $k$ --- индексы алгоритмов, $M_i(j)$ --- производительность $i$-го алгоритма на $j$-ом наборе данных и $I_{\{M_i(j)>M_k(j)\}}$ --- индикаторная функция, такая что
\[
I_{\{M_i(j)>M_k(j)\}}= \begin{cases}1 & \text { если оба определены и } M_i(j)>M_k(j) \\ 0 & \text { иначе }\end{cases}.
\]

Будем использовать нормализованное <<выигрышное число>>, для удобства интерпретации результатов, определённое как:
\begin{equation}
	\label{eq:wn2}
	NWN_i(M)=\frac{WN_i(M)}{IWN_i(M)},
\end{equation}
где $IWN_i(M)$ --- идеальное <<выигрышное число>>, то есть теоретически наибольшее число, которое было
бы у алгоритма в случае, если бы он был наиболее точным. Определяется как:
\begin{equation}
	\label{eq:wn3}
	\mathrm{IWN}_i(M)=\sum_{j=1}^n \sum_{k=1}^m D_{\{M_i(j)>M_k(j)\}},
\end{equation}
где индикаторная функция имеет значение
\[
D_{\{M_i(j)>M_k(j)\}}= \begin{cases}1 & \text { если оба определены } \\ 0 & \text { иначе }\end{cases}.
\]

Формулы~(\ref{eq:wn1})--(\ref{eq:wn3})~\cite{cmp}.

Результаты замеров метрик взяты из исследования Ника Такса~\cite{cmp} и приведены приведены в таблице~\ref{tbl:measure}. Данная таблица показывает <<выигрышное число>> и количество наборов данных, используемых для сравнения, конкретного алгоритма для различных метрик ранжирования.

\begin{table}[h]
	\begin{center}
		\begin{threeparttable}
			\captionsetup{justification=raggedright,singlelinecheck=off}
			\caption{Результаты замеров производительности методов обучения ранжированию}
			\label{tbl:measure}
			\begin{tabular}{|l|c|c|c|c|c|c|c|c|}
				\hline
				 & \multicolumn{2}{c|}{NDCG@3} & \multicolumn{2}{c|}{NDCG@5} & \multicolumn{2}{c|}{NDCG@10} & \multicolumn{2}{c|}{MAP} \\\hline
				 Метод & NWN  & к.д. & NWN  & к.д & NWN  & к.д & NWN  & к.д. \\\hline
				Linear Regression & 0.0754 & 9 & 0.1099& 9& 0.0829& 8& 0.0650& 8 \\\hline
				Ranking SVM &  0.3014 & 12& 0.3613& 11& 0.4496& 17& 0.3400& 13 \\\hline
				LambdaRank &  0.2000 &1 &0.2000 &1 &0.5714& 2 &-& - \\\hline
				ListNet &  0.4480 &12 &0.4911 &12 &0.5982 &12 &0.4504 &12 \\\hline
			\end{tabular}
		\end{threeparttable}
	\end{center}
\end{table}

\section{Вывод}

В рамках выбранных методов лучше всех себя показал ListNet по всем метрикам, так как его <<выигрышные числа>> самые высокие. Хотя по сравнению с остальными алгоритмами из~\cite{cmp} он является довольно средним, поскольку все его NWN колеблется рядом с 0.5, то есть превосходит примерно половину методов, с которыми он сравнивался по всем метрикам . Linear Regression же проигрывает почти девяноста процентам всех алгоритмов, с которыми его сравнивали. Важно отметить, что данная метрика является нормированной.

Алгоритмы Ranking SVM, LambdaRank и ListNet лучше ранжируют большой объём документов, но плохо справляются с сайтами в самом топе поисковой выдачи в отличие от остальных алгоритмов в исследовании~\cite{cmp}. Данный вывод был сделан, поскольку метрика NDCG будет увеличиваться прямопропорционально учитываемому  количеству сайтов. Это особенно заметно у  LambdaRank. Также эти методы обладают не лучшей точностью поисковой выдачи. В итоге можно понять, что выбранные алгоритмы являются средними по показателям,поскольку практически все их метрики меньше 0.5. При этом они довольно сбалансированы, так как нет большой просадки в значениях <<выигрышных чисел>>.